## Day 1

### 课程学习到的内容：

#### **CPU和GPU区别：**

1. CPU有**较大内存**（或本地缓存），而GPU往往有**较小内存**；

2. 与CPU相比，GPU有**更多的寄存器**，来支持大量线程；

3. GPU有更多的SIMD执行单元，而CPU具有的SIMD单元较少。

   注：SIMD（单指令多数据）是一种并行计算技术，允许一个指令同时操作多个数据元素。而SISD（单指令单数据）中一个指令只能操作一个数据元素。

#### 影响GPU性能的几个相关参数：

计算单元的数量、内核数、线程数等。



### 一些概念的记录

#### **GPU的硬件架构：**

- **VRAM**:即显存,用于存储数据。GPU的VRAM**带宽很高**,但**延迟较大**。
- SRAM:即共享内存和寄存器,用于缓存频繁访问的数据。SRAM**容量较小**但**访存延迟非常低**。

#### **GPU上实现高性能计算的几个挑战：**

1. **内存访问优化**:即对齐和合并对DRAM的访问,避免带宽浪费。需要仔细规划数据布局,使得同一个warp(32个线程)的内存请求能够合并。

2. **共享内存管理**:合理利用SRAM,将频繁访问的数据块缓存到共享内存中,减少对DRAM的访问。但需要小心管理,避免bank冲突。

3. **寄存器分配**:寄存器是GPU上最快的存储,但每个线程可用的寄存器数量非常有限。需要精心设计算法,尽量重用寄存器。

4. **线程束调度**:要最大化GPU的占用率,需要让尽量多的线程束在GPU上并发执行。这就要求仔细规划线程数量和线程束大小,权衡并行性和寄存器、共享内存的使用量。

5. **指令优化**:GPU也有自己的指令集和流水线,不同的指令延迟和吞吐量不同。比如要最大化Tensor Core的使用。

#### 异构并行计算的概念：

“并行计算”的核心思想：将一个大的计算任务分解成许多小的子任务，然后**同时**处理这些子任务，从而显著缩短总计算时间。

“异构”的核心思想：在计算系统中，使用**不同类型**的处理单元来协同工作。这些处理单元各有专长，适合处理不同的任务。

**“异构” vs “同构”**：

- **同构计算**：系统里所有的处理器都是一样的。例如，一台只有多核CPU的服务器，所有核心都是相同的x86架构。
- **异构计算**：系统里包含不同架构的处理器。最常见的组合就是 **CPU + GPU**。此外，还可能包括专用的加速卡（如FPGA、ASIC等，例如Google的TPU）。

**异构并行计算**就是指：将一个计算任务合理地拆分，并分配给系统中不同类型的处理单元（最典型的是CPU和GPU），让它们同时并行工作，发挥各自最大的优势，以达到极高的整体计算效率。

#### CPU内核和线程的概念：

**内核**：即CPU上能够同时处理数据的物理内核数。每个内核都具备**独立执行计算任务**的能力。

**线程**：在一个内核上能够同时处理的多项任务。

注：在早期的CPU设计中，通常是1个核心对应1个线程。然而，随着Intel超线程技术的引入，1个核心现在能够对应2个线程。



### 几个疑问

#### 疑问1：我们一般在显卡上跑模型时，如果模型运行所需要的显存大于显卡上的显存容量，那么就会出现内存溢出报错。如果模型正常运行，那么此时是不是GPU上的所有线程都在并行计算？

Deepseek的回答：
首先区分两个概念：显存和计算单元：

- **显存（Video Memory / GPU Memory）**：这是GPU的**数据仓库**。它存储了模型权重、激活值、输入输出数据等。显存不足（OOM）意味着“仓库”太小，装不下所有需要的“货物”，所以作业无法开始。
- **计算单元（CUDA Cores/Stream Processors）**：这是GPU的**加工车间**，里面是真正进行数学运算（乘加）的工人（线程）。您的问题主要关于这部分。

“显存足够”只是保证了“仓库”里的货物可以顺利提供给“车间”，但“车间”里的工人（线程）是否都在一刻不停地工作，则是另一个问题。

GPU实现极高并行度的方式不是让所有线程同时开始、同时结束，而是通过**极速切换**来隐藏延迟，从而在宏观上保持极高的利用率。以下是导致线程不能时刻计算的几个主要原因：

 **1. warp/wavefront 的锁步执行**

- GPU线程被组织成组来管理。在NVIDIA GPU中，32个线程为一组，称为一个 **Warp**；在AMD GPU中，64个线程为一组，称为一个 **Wavefront**。
- 一个Warp中的32个线程**在同一周期内必须执行相同的指令**。这意味着它们就像一支步伐统一的军队齐步走。

**2. 内存延迟**

- 这是导致线程空闲的最主要原因。GPU的计算速度极快，但访问显存（哪怕是高速的GDDR/HBM显存）相对而言非常慢。
- 当线程需要读取显存中的数据（如下一层权重、输入数据）时，它必须等待数据从显存传输到计算单元。这个过程需要几百个时钟周期。
- 在这段漫长的等待时间里，该线程所在的Warp**不会被分配任何计算任务**，整个Warp的状态会被挂起。GPU的硬件调度器会立刻切换到另一个**已经准备好数据、可以立即计算**的Warp上继续工作。
- GPU正是通过拥有**远超其物理核心数量的线程（Warp）**，来保证总有足够多的Warp可以切换，从而让计算单元一直有活干，从而“隐藏”了内存访问的延迟。
- 所以，在任何一瞬间，总有一部分Warp因为等待数据而处于“闲置”状态。

#### 疑问2：为什么深度学习代码中的“num_workers”参数在windows最好设置成0，否则会报线程错误，而在linux系统可以随便设置？

Deepseek的回答：

根本原因在于 **Windows 和 Linux/macOS 创建新进程的方式完全不同**。这个 `num_workers` 参数控制的是 PyTorch 的 `DataLoader` 使用多少个子进程来预加载数据（数据预处理、增强等）。多进程是为了避免数据准备成为模型训练速度的瓶颈。

**深入解析：`fork` (Linux) 与 `spawn` (Windows)**

**1. 在 Linux/macOS 上：使用 `fork`**

- **fork 做了什么？** 当 Python 使用 `fork()` 系统调用创建一个新进程时，新产生的子进程几乎是父进程的一个**完整副本**。子进程会获得父进程所有资源（如内存空间、文件描述符、环境变量等）的**拷贝**。
- **为什么可以“随便设置”？**：因为 `fork` 非常轻量和快速，子进程的起点状态高度一致且已知，不容易出错。只要你的代码没有特别诡异的全局状态依赖，多进程就能顺利工作。

**2. 在 Windows 上：使用 `spawn**`

- **spawn 做了什么？** Windows 没有 `fork()` 系统调用。它创建新进程的方式是 **spawn**。这个过程是：
  1. 一个新的 Python 解释器进程被启动。
  2. 这个全新的进程是“干净的”，它**只继承了非常有限的信息**。
  3. 它需要重新执行当前的主脚本，并**重新导入**所有需要的模块，直到它运行到目标函数（即 `DataLoader` 的 worker 函数）为止。
- **对深度学习的影响**：
  - **重新执行主脚本**：这是最关键的区别。如果你的主脚本（通常是 `main.py` 或 `train.py`）中有**不被 if __name__ == '__main__': 保护**的代码，这些代码在子进程重新导入时又会被执行一遍。

**为什么设置 `num_workers=0` 可以解决问题？**

- `num_workers=0` 意味着 **“在主进程中进行所有数据加载”**。
- 这样就不会创建任何新的子进程，完全避免了 `spawn` 带来的所有问题：没有进程创建，没有脚本重新执行，没有CUDA重复初始化。

#### 疑问3：既然GPU上SRAM访问延迟小，那为什么不提高SRAM的容量？让更多的计算数据缓存到SRAM中？

Deepseek的回答：

提高SRAM容量会急剧增加芯片面积（成本）和功耗，同时会降低访问速度，这与GPU的设计目标（高吞吐量、能效比）背道而驰。